Traditionally, distributed high-throughput \- computing (HTC), such as grid
computing, was the primary modality for analysis of large data volumes for
experiments in fields of High Energy and Nuclear Physics.
Increasingly however, high-performance computing (HPC) resources are being
used to support the computing requirements, which mandate different
approaches to workload management for computational campaigns that require
many jobs to be executed over a long duration.
% Traditionally distributed (grid) computing was the primary modality for
% analysis of large data volumes for experiments such as ATLAS and others at
% the Large Hadron Collider (LHC). Increasingly however, high-performance
% computing (HPC) resources are being used to support the computing
% requirements of ATLAS and other experiments.
% Given the difference in resource allocation and utilization approaches between
% HTC and HPC resources, this poses interesting challenges for the experiments..
% For supercomputing facilities, ATLAS represents novel workloads properties and
% execution requirements, viz., long running campaigns with many tasks and an
% opportunity to improve task throughput by flexible job sizing and task packing
A campaign execution strategy is described as the selection of parameters
defining the number of jobs, along with their size, duration and distribution.
The aim of this paper is to investigate and explore campaign execution
strategies with the objective of maximizing the probability of utilizing a
given allocation as measured by the number of $cores \times hours$ consumed
over a given period.
We propose a simplified quantitative model and a workflow simulator to compute
the probability of utilizing a given allocation.
The model was validated on both synthetic and real log data over several 
months of a DOE Leadership Computing Facility resources (the Titan 
supercomputer), with emphasis on data about utilization of resources by 
the workload management system PanDA, to identify possible campaign 
execution strategies and compare them with others.
% Identified strategies were compared with other possible strategies. 
Experiments conducted using the simulator, showed that in most cases
identified strategies increase the probability of utilizing an allocation
faster than a random choice of job parameters.


% In the current era of compute-intensive applications and exa-scale
% processings, HPCs and supercomputers along with such technologies as grid
% (HTC) and cloud computing play an important role. Thus the organization of
% the calculation and execution processes within these instruments require a
% particular attention. Sharing of computing resources between its clients
% such as individual users and groups that represent certain projects is
% determined by predefined usage policies, resource quota per user/group and
% its dynamic workload based on usage activities. Thus, the load on a
% supercomputer depends on the number and parameters of computing jobs
% running there: the number of required nodes, required execution time
% (walltime), and jobs generation rate. Predefined job parameters such as its
% number, size, length, rate, are referred as an execution strategy.
% The aim of this work is to identify execution strategies geared towards the
% goal of maximizing the probability of utilization of allocated resources per
% defined project on a supercomputer in a given time period. Resources
% allocation is a number of provided cores or computing nodes for a limited
% time ($cores \times hours$), also mentioned as an allocation time. This work
% also gives a possibility to estimate a potential resource utilization based
% on provided job parameters for a given time period and the current
% supercomputer workload. A simplified model for utilization of allocation
% time and a simulator based on Queueing Theory were designed. The model was
% tested on both synthetic and real log data over several months of the
% supercomputer work (the Titan supercomputer work was examined), and
% identified strategies were compared with other possible strategies.
% Experiments conducted using the simulator, showed that in most cases
% identified strategies increase the probability of utilizing allocation
% faster than a random choice of job processing parameters.
